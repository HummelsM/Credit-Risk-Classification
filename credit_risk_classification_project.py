# -*- coding: utf-8 -*-
"""Credit Risk Classification Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XYRSafq1LhDw1j_MCR9EE2J2m-j5Sz5V

### **Loading Libraries**
"""

import pandas as pd
import plotly.express as px
import matplotlib.pyplot as plt
import datetime
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
import seaborn as sns

from sklearn.model_selection import cross_val_score
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
import numpy as np

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

credit=pd.read_csv("credit_risk.csv")
credit.head()

"""# **Data Preprocessing**"""

credit.shape
credit.info()
credit.describe()

credit.isna().any()
credit.isna().sum()

credit.rename(columns = {"'checking_status'":'Checking_Status', "'duration'":'Duration',"'credit_history'":'Credit_History',"'purpose'":'Purpose',
                         "'credit_amount'":'Credit_Amount',"'savings_status'":'Savings_Status',"'employment'":'Employment',"'installment_commitment'":'Installment_Commitment',
                         "'personal_status'":'Relationship_Status',"'other_parties'":'Other_Parties',"'residence_since'":'Time_Owned',"'property_magnitude'":'Property_Type',
                         "'age'":'Age',"'other_payment_plans'":'Other_Payment_Plans',"'housing'":'Living_Status',"'existing_credits'":'Existing_Credits',"'job'":'Job',
                         "'num_dependents'":'Dependents',"'own_telephone'":'Telephone',"'foreign_worker'":'Foreign_Worker',"'class'":'Credit_Risk'
                         }, inplace = True)

credit['Credit_Risk'].value_counts()

credit.drop(['id','Telephone','Foreign_Worker','Relationship_Status'], axis=1,inplace=True)

credit['Property_Type']=credit['Property_Type'].map({
    'car':2,
    'no known property':3,
    'real estate': 1,
    'life insurance':1
})

credit['Purpose'] = credit['Purpose'].map({
    'radio/tv': 1,
    'new car': 1,
    'furniture/equipment': 1,
    'domestic appliance': 1,
    'repairs': 1,
    'retraining': 1,
    'business': 2,
    'education': 2,
    'used car': 2,
    'other':2
})

credit['Savings_Status'] = credit['Savings_Status'].map({
    '<100': 3,
    'no known savings': 3,
    '100<=X<500': 2,
    '500<=X<1000': 1,
    '>=1000': 1
})

credit['Employment'] = credit['Employment'].map({
    'unemployed': 4,
    '<1': 3,
    '1<=X<4': 2,
    '4<=X<7': 2,
    '>=7': 1
})

credit['Checking_Status'] = credit['Checking_Status'].map({
    'no checking':2,
    '<0': 2,
    '0<=X<200': 1,
    '>=200': 1
})

credit['Credit_History'] = credit['Credit_History'].map({
    'existing paid': 1,
    'critical/other existing credit	': 2,
    'delayed previously': 2,
    'no credits/all paid': 1,
    'all paid': 1
})

credit['Other_Parties'] = credit['Other_Parties'].map({
    'none': 2,
    'guarantor': 1,
    'co applicant': 1
})

credit['Other_Payment_Plans'] = credit['Other_Payment_Plans'].map({
    'none': 2,
    'bank': 1,
    'stores':1
})

credit['Living_Status'] = credit['Living_Status'].map({
    'own': 1,
    'rent': 2,
    'for free':3
})

credit['Job'] = credit['Job'].map({
    'skilled': 1,
    'unskilled resident': 2,
    'high qualif/self emp/mgmt	':1,
    'unemp/unskilled non res':2
})

credit['Credit_Risk'] = credit['Credit_Risk'].map({
    'good': 0,
    'bad': 1,
})

credit.drop_duplicates(inplace=True)
credit.fillna(credit.mean(),inplace=True)

def remove_outliers(df):
    for column in df.select_dtypes(include=['int64']).columns:
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
    return df

remove_outliers(credit)

scaler = StandardScaler()
credit[['Age', 'Credit_Amount','Installment_Commitment','Existing_Credits']] = scaler.fit_transform(credit[['Age','Credit_Amount','Installment_Commitment','Existing_Credits']])

"""# **Cluster Analysis and Model Training**"""

# Assuming df is the DataFrame with both numerical and manually ranked categorical features
columns_to_cluster = [col for col in credit.columns if col != 'Credit_Risk']
correlation_matrix = credit[columns_to_cluster].corr(method='spearman')

correlation_matrix

from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
import seaborn as sns
import matplotlib.pyplot as plt

# Perform hierarchical clustering
#linkage_matrix = linkage(correlation_matrix, method='ward')

correlation_matrix = correlation_matrix.replace([np.inf, -np.inf], np.nan)  # Replace inf with NaN
correlation_matrix = correlation_matrix.fillna(correlation_matrix.max().max())  # Replace NaN with maximum value in matrix
#Now proceed with the hierarchical clustering
linkage_matrix = linkage(correlation_matrix, method='ward')

# Visualize the dendrogram
plt.figure(figsize=(10, 7))
dendrogram(linkage_matrix, labels=correlation_matrix.columns)
plt.show()

# Decide the number of clusters and assign features to clusters
num_clusters =2   # Change this based on the dendrogram
clusters = fcluster(linkage_matrix, num_clusters, criterion='maxclust')
feature_clusters= {cluster: [] for cluster in range(1, num_clusters + 1)}
for idx, cluster in enumerate(clusters):
    feature_clusters[cluster].append(correlation_matrix.columns[idx])

plt.figure(figsize=(12, 7))  # Increase the figure size
dendrogram(linkage_matrix, labels=correlation_matrix.columns, leaf_rotation=90)  # Rotate the labels
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Split data into train-test
X = credit.drop('Credit_Risk', axis=1)
y = credit['Credit_Risk']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# Create cluster-specific datasets
cluster_data = {}
for cluster_id, features in feature_clusters.items():
    valid_features = [f for f in features if f in X_train.columns]
    cluster_data[cluster_id] = {
        'X_train': X_train[valid_features],
        'X_test': X_test[valid_features],
        'y_train': y_train,
        'y_test': y_test
    }

# Train model directly on the clusters
for cluster_id, data in cluster_data.items():
    print(f"\nCluster {cluster_id}: Features {data['X_train'].columns.tolist()}")

    # Train model
    model = RandomForestClassifier(random_state=42)
    model.fit(data['X_train'], data['y_train'])

    # Predict and evaluate
    y_pred = model.predict(data['X_test'])
    y_pred_prob = model.predict_proba(data['X_test'])[:, 1]
    print(f"Classification Report for Cluster {cluster_id}:")
    print(classification_report(data['y_test'], y_pred))
    auc = roc_auc_score(data['y_test'], y_pred_prob)
    print(f"ROC AUC for Cluster {cluster_id}: {auc:.2f}")

    # Plot ROC Curve
    fpr, tpr, _ = roc_curve(data['y_test'], y_pred_prob)
    plt.plot(fpr, tpr, label=f'Cluster {cluster_id} (AUC = {auc:.2f})')

# Finalize the plot
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve by Cluster')
plt.legend()
plt.show()

from sklearn.inspection import permutation_importance


# Feature Importance Using Permutation
feature_impact_by_cluster = {}
for cluster_id, data in cluster_data.items():
    cluster_model = RandomForestClassifier(random_state=42)
    cluster_model.fit(data['X_train'], data['y_train'])
    importance = permutation_importance(
        cluster_model, data['X_test'], data['y_test'], scoring='f1', n_repeats=5, random_state=42
    )
    feature_impact = {
        feature: importance.importances_mean[i]
        for i, feature in enumerate(data['X_test'].columns)
    }
    feature_impact_sorted = sorted(feature_impact.items(), key=lambda x: x[1], reverse=True)
    feature_impact_by_cluster[cluster_id] = feature_impact_sorted

    print(f"\nCluster {cluster_id} Feature Impact:")
    for feature, impact in feature_impact_sorted:
        print(f"  Feature: {feature}, Impact: {impact:.4f}")

import seaborn as sns

# Example: Plot for Cluster 1
cluster_id = 1
impacts = feature_impact_by_cluster[cluster_id]
features = [item[0] for item in impacts]
scores = [item[1] for item in impacts]

# Create bar plot
plt.figure(figsize=(10, 6))
sns.barplot(x=scores, y=features, orient='h', palette='viridis')
plt.title(f"Feature Impact for Cluster {cluster_id}")
plt.xlabel("Impact (Importance Score)")
plt.ylabel("Features")
plt.show()

import seaborn as sns

# Example: Plot for Cluster 2
cluster_id = 2
impacts = feature_impact_by_cluster[cluster_id]
features = [item[0] for item in impacts]
scores = [item[1] for item in impacts]

# Create bar plot
plt.figure(figsize=(10, 6))
sns.barplot(x=scores, y=features, orient='h', palette='viridis')
plt.title(f"Feature Impact for Cluster {cluster_id}")
plt.xlabel("Impact (Importance Score)")
plt.ylabel("Features")
plt.show()

import pandas as pd
import seaborn as sns

# Convert feature impacts into a DataFrame
heatmap_data = pd.DataFrame.from_dict(
    {cluster_id: {feature: impact for feature, impact in impacts} for cluster_id, impacts in feature_impact_by_cluster.items()},
    orient='index'
).fillna(0)  # Fill missing values with 0

# Plot heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(heatmap_data.T, annot=True, cmap="coolwarm", cbar=True)
plt.title("Feature Impact Heatmap Across Clusters")
plt.xlabel("Clusters")
plt.ylabel("Features")
plt.show()

# Combine Predictions and Evaluate
final_predictions = []
for cluster_id, data in cluster_data.items():
    model = RandomForestClassifier(random_state=42)
    model.fit(data['X_train'], data['y_train'])
    preds = model.predict(data['X_test'])
    final_predictions.append(preds)

# Aggregate predictions (majority vote)
final_predictions_combined = np.round(np.mean(final_predictions, axis=0))
print("\nCombined Classification Report:")
print(classification_report(y_test, final_predictions_combined))

"""# **Comparison between results from combined cluster predictions and a model without using clustering**"""

# Define features and target
new_X = credit.drop(['Credit_Risk'], axis=1)  # Features
new_y = credit['Credit_Risk']                            # Target variable

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(new_X, new_y, test_size=0.1, random_state=42)
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

# Predict and evaluate
y_pred =rf.predict(X_test)
acc=accuracy_score(y_test, y_pred)
print(f"Accuracy: {acc:.2f}")

# Compute ROC AUC for cluster-based and non-clustered predictions
cluster_auc = roc_auc_score(y_test, final_predictions_combined)
non_cluster_auc = roc_auc_score(y_test, y_pred_prob)
# Non-clustered predictions
fpr_non_cluster, tpr_non_cluster, _ = roc_curve(y_test, y_pred_prob)

# Combine cluster-based predictions (using majority vote as an example)
# Convert `final_predictions` (list of arrays) to a 2D NumPy array
final_predictions_array = np.array(final_predictions)  # Shape: (num_clusters, num_samples)

# Majority voting for the combined prediction
combined_cluster_preds = np.round(final_predictions_array.mean(axis=0)).astype(int)  # Shape: (num_samples,)

# Plot ROC curves
# Cluster-based predictions
fpr_cluster, tpr_cluster, _ = roc_curve(y_test, combined_cluster_preds)

plt.figure(figsize=(10, 7))
plt.plot(fpr_cluster, tpr_cluster, label=f'Cluster-Based Model (AUC = {cluster_auc:.2f})', linestyle='--')
plt.plot(fpr_non_cluster, tpr_non_cluster, label=f'Non-Clustered Model (AUC = {non_cluster_auc:.2f})', linestyle='-')

# Random classifier line
plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.50)')

# Finalize plot
plt.title('ROC Curve Comparison')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Calculate metrics for the combined clustered model
final_combined_preds = np.concatenate(final_predictions)  # Combine predictions from all clusters
final_y_test = np.concatenate([data['y_test'] for data in cluster_data.values()])  # Combine the true labels

# Metrics for combined cluster-based model
clustered_accuracy = accuracy_score(final_y_test, final_combined_preds)
clustered_precision = precision_score(final_y_test, final_combined_preds, average='binary')
clustered_recall = recall_score(final_y_test, final_combined_preds, average='binary')
clustered_f1 = f1_score(final_y_test, final_combined_preds, average='binary')

print("Metrics for Combined Cluster-Based Model:")
print(f"Accuracy: {clustered_accuracy:.2f}")
print(f"Precision: {clustered_precision:.2f}")
print(f"Recall: {clustered_recall:.2f}")
print(f"F1 Score: {clustered_f1:.2f}")

# Metrics for the unclustered model
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

unclustered_accuracy = accuracy_score(y_test, y_pred)
unclustered_precision = precision_score(y_test, y_pred, average='binary')
unclustered_recall = recall_score(y_test, y_pred, average='binary')
unclustered_f1 = f1_score(y_test, y_pred, average='binary')

print("\nMetrics for Unclustered Model:")
print(f"Accuracy: {unclustered_accuracy:.2f}")
print(f"Precision: {unclustered_precision:.2f}")
print(f"Recall: {unclustered_recall:.2f}")
print(f"F1 Score: {unclustered_f1:.2f}")